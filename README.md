# **Bootcamp de Inteligencia Artificial con TensorFlow**  
### **Por Diego Vallejo**  

## **Descripción General**  
Este bootcamp está diseñado para proporcionar una base sólida en redes neuronales artificiales utilizando TensorFlow, desde los modelos más básicos hasta arquitecturas avanzadas como redes convolucionales y recurrentes. A través de 12 lecciones con ejemplos prácticos en código, los participantes aprenderán la teoría detrás de los modelos clásicos y su implementación en TensorFlow.  

## **Objetivos**  
- Comprender los fundamentos de las redes neuronales artificiales.  
- Explorar la evolución histórica y los avances en inteligencia artificial.  
- Implementar modelos neuronales desde cero con TensorFlow.  
- Aplicar técnicas avanzadas como retropropagación, optimización y redes convolucionales.  

---

## **Temario**  

### **Módulo 1: Fundamentos de Redes Neuronales**  

#### **Lección 1: Introducción a TensorFlow y las Redes Neuronales**  
- ¿Qué es TensorFlow? Instalación y primeros pasos.  
- Introducción a los tensores y operaciones básicas.  
- **Ejemplo:** Creación de un tensor y operaciones matemáticas en TensorFlow.  

#### **Lección 2: Neurona Artificial – McCulloch y Pitts (1943)**  
- Concepto de neurona artificial y su importancia en la IA.  
- Funciones de activación: Sigmoide, ReLU, Tanh, Softmax.  
- **Ejemplo:** Implementación de una neurona artificial en TensorFlow.  

#### **Lección 3: Perceptrón – Frank Rosenblatt (1959)**  
- Arquitectura y funcionamiento del perceptrón.  
- Regla de aprendizaje y ajuste de pesos.  
- **Ejemplo:** Implementación de compuertas lógicas (AND, OR) con un perceptrón en TensorFlow.  

#### **Lección 4: Adaline y la Regla de Hebb – Widrow & Hoff (1960)**  
- Diferencias entre perceptrón y Adaline.  
- Corrección del error y aprendizaje con regla de Hebb.  
- **Ejemplo:** Implementación de Adaline en TensorFlow para un problema de regresión.  

---

### **Módulo 2: Redes Neuronales Multicapa y Aprendizaje Profundo**  

#### **Lección 5: Perceptrón Multicapa (MLP) – Minsky y Papert (1969)**  
- Limitaciones del perceptrón simple.  
- Introducción a las redes multicapa.  
- **Ejemplo:** Implementación de un MLP para clasificación en TensorFlow.  

#### **Lección 6: Retropropagación del Error – Rumelhart, Hinton & Williams (1986)**  
- Explicación del algoritmo de retropropagación.  
- Cálculo del gradiente y ajuste de pesos.  
- **Ejemplo:** Implementación de una red con backpropagation en TensorFlow.  

#### **Lección 7: Algoritmo de Levenberg-Marquardt (1979)**  
- Explicación del método Levenberg-Marquardt.  
- Comparación con otros optimizadores (SGD, Adam).  
- **Ejemplo:** Implementación de una red con Levenberg-Marquardt en TensorFlow.  

---

### **Módulo 3: Redes Neuronales Avanzadas**  

#### **Lección 8: Redes Neuronales Recurrentes (RNNs) – Hopfield (1982), Elman (1990)**  
- ¿Qué son las RNNs y para qué se usan?  
- Backpropagation through time (BPTT).  
- **Ejemplo:** Implementación de una RNN en TensorFlow para predicción de series temporales.  

#### **Lección 9: Modelo de Hopfield (1982)**  
- Introducción a redes neuronales asociativas.  
- Propiedades de convergencia y memorias atractoras.  
- **Ejemplo:** Implementación de una red de Hopfield en TensorFlow.  

#### **Lección 10: Autoencoders – Hinton & Salakhutdinov (2006)**  
- Arquitectura y aplicaciones de los autoencoders.  
- Reducción de dimensionalidad y aprendizaje no supervisado.  
- **Ejemplo:** Implementación de un autoencoder para compresión de datos en TensorFlow.  

---

### **Módulo 4: Redes Neuronales Convolucionales y Aplicaciones Modernas**  

#### **Lección 11: Redes Convolucionales (CNNs) – Fukushima (1980), LeCun (1998)**  
- Arquitectura de una CNN: Convolución, Pooling, Fully Connected Layers.  
- Aplicaciones en visión artificial.  
- **Ejemplo:** Implementación de una CNN para clasificación de imágenes (MNIST o CIFAR-10) en TensorFlow.  

#### **Lección 12: Proyecto Final: Clasificación con Redes Neuronales**  
- Construcción de una red completa para un problema real.  
- Explicación de la selección de hiperparámetros y optimización.  
- **Ejemplo:** Implementación de una red para detección de spam o reconocimiento de dígitos.  

---

## **Referencias y Material Adicional**  
- McCulloch, W. S., & Pitts, W. (1943). *A logical calculus of the ideas immanent in nervous activity.*  
- Rosenblatt, F. (1959). *The perceptron: A probabilistic model for information storage and organization in the brain.*  
- Minsky, M., & Papert, S. (1969). *Perceptrons: An Introduction to Computational Geometry.*  
- Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations by back-propagating errors.*  
- Fukushima, K. (1980). *Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.*  
- LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-based learning applied to document recognition.*  

---

## **Formato y Requisitos**  
- **Nivel:** Principiante - Intermedio  
- **Requisitos:** Conocimientos básicos de Python y álgebra lineal.  
- **Herramientas:** TensorFlow, Keras, Jupyter Notebooks.  
- **Duración:** 12 sesiones (aproximadamente 4 semanas).  

---

## **Conclusión**  
Este bootcamp proporcionará una base teórica y práctica para entender cómo funcionan las redes neuronales y cómo implementarlas en TensorFlow. Al final del curso, los participantes tendrán la capacidad de construir modelos de IA desde cero y aplicarlos en problemas reales.
